{"cells":[{"cell_type":"markdown","metadata":{"id":"ZWNWttSBh5vf"},"source":["# Multimodal Object Detection by Channel-Switching and Spatial Attention\n","\n","**Directions for Use**: \\\n","This notebook is self contained and was tested on a V100 High-Ram GPU on Google Colab. Simply run each cell sequentially, and the model will train using the hyperparameters given in the configuration dictionary a few cells down. \n","\n","If anything looks unfamiliar, please work through **model-train.ipynb** first, as it is more verbose and will likely clear a lot of your confusions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131969,"status":"ok","timestamp":1714098722689,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"rbZLH8brgL6p","outputId":"98c2b1d9-9a09-48e6-c396-6a653ae94b19"},"outputs":[],"source":["%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q\n","%pip install xmltodict -q \n","!pip install wandb --quiet\n","!pip install wget -q\n","%pip install torchinfo -q\n","%pip install torchmetrics -q"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1700,"status":"ok","timestamp":1714098724385,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"6oQeBdMlgRTR"},"outputs":[],"source":["import torch\n","import unittest\n","import random\n","from torchinfo import summary\n","import os\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import xmltodict\n","from PIL import Image\n","import tqdm\n","import wandb\n","\n","import torchvision.transforms.functional as F\n","from PIL import ImageDraw\n","from PIL.ImageShow import IPythonViewer"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1600852,"status":"ok","timestamp":1714100325490,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"ew7ufno5gVKE","outputId":"d3a0e80a-3b4e-4894-df55-54c896ab4fda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"{DEVICE=}\")\n","\n","from google.colab import drive # Link to your drive if you are not using Colab with GCP\n","drive.mount('/content/drive')\n","\n","!cp [fill me in] [fill me in]\n","!unzip -q 'LLVIP.zip' -d '/content/data'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714100420182,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"tzphEo3RgaiZ"},"outputs":[],"source":["config = {\n","    'lr': 2.5e-4,\n","    'num_classes': 2,\n","    'reduction_factor': 4,\n","    'fpn_feature_dim': 256,\n","    'channel_sizes': [256, 512, 1024, 2048],\n","    'batch_size': 3,\n","    'epochs': 10,\n","    'rpn_gamma': 10,\n","    'fcnn_gamma': 10,\n","    'checkpoints_path': 'fill me in ',\n","    'best_mAP': 0,\n","    'run_name': 'fill me in '\n","    }"]},{"cell_type":"markdown","metadata":{"id":"OvK2UnY6bqTC"},"source":["## Dataloader"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1714100420183,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"w07u9UvcbrQQ"},"outputs":[],"source":["class LLVIPDataset(Dataset):\n","    def __init__(self, root, rgb_mean=None, rgb_std=None, ir_mean=None, ir_std=None, partition=\"train\", val_set={}, split=0.9, flips=True):\n","\n","        self.partition = \"train\"\n","        self.flips = flips\n","\n","        rgb_dir = os.path.join( root + \"visible/\" + partition + \"/\")\n","        ir_dir = os.path.join( root + \"infrared/\" + partition + \"/\")\n","\n","        if partition == \"valid\" and val_set:\n","\n","            rgb_paths = val_set[\"rgb\"]\n","            ir_paths = val_set[\"ir\"]\n","\n","        elif partition == \"valid\" and not val_set:\n","            raise ValueError(\"Cannot make validation set without val_set arg!\")\n","\n","        else:\n","\n","            rgb_paths = [os.path.join(rgb_dir + file) for file in sorted(os.listdir(rgb_dir))]\n","            ir_paths = [os.path.join(ir_dir + file) for file in sorted(os.listdir(ir_dir))]\n","\n","            split_idx = int(len(rgb_paths) * split)\n","\n","            self.val_set = {}\n","            rgb_paths, self.val_set[\"rgb\"] = rgb_paths[:split_idx], rgb_paths[split_idx:]\n","            ir_paths, self.val_set[\"ir\"] = ir_paths[:split_idx], ir_paths[split_idx:]\n","\n","        self.data = []\n","        self.new_size = (640, 512)\n","        if rgb_mean and rgb_std and ir_mean and ir_std:\n","          self.rgb_normalize = torchvision.transforms.Normalize(rgb_mean, rgb_std)\n","          self.ir_normalize = torchvision.transforms.Normalize(ir_mean, ir_std)\n","          self.normalize = True\n","        else:\n","          print(\"No normalization of the images!\")\n","          self.normalize = False\n","\n","        self.to_tensor = torchvision.transforms.ToTensor()\n","\n","        self.orig_size = self.to_tensor(Image.open(rgb_paths[0])).size()[1:]\n","        assert self.orig_size == self.to_tensor(Image.open(ir_paths[0])).size()[1:]\n","\n","        loading_bar = tqdm.tqdm(total=len(rgb_paths), desc=f\"Building {partition} dataset\")\n","        for rgb_file, ir_file in zip(rgb_paths, ir_paths):\n","\n","            base = os.path.splitext(os.path.basename(rgb_file))[0]\n","            annotation_file = os.path.join(root + \"Annotations/\" + base + \".xml\")\n","\n","            bboxes = []\n","            classes = []\n","\n","            with open(annotation_file, \"r\") as file:\n","                annotation_dict = xmltodict.parse(file.read())\n","                bboxes, classes = self.get_bboxes(annotation_dict[\"annotation\"])\n","            file.close()\n","\n","            if not bboxes and not classes:\n","                tqdm.tqdm.write(f\"Skipping File {base}, with no annotations.\")\n","                loading_bar.update()\n","                continue\n","            else:\n","                bboxes = torch.FloatTensor(bboxes)\n","                classes = torch.LongTensor(classes).type(torch.int64)\n","\n","            self.data.append({\n","                \"base\": base,\n","                \"rgb\": rgb_file,\n","                \"ir\": ir_file,\n","                \"bboxes\": bboxes,\n","                \"classes\": classes,\n","            })\n","\n","            loading_bar.update()\n","\n","    def map_idx(self, idx, max, new_max, min=0, new_min=0):\n","        # Convert the old range into the new range\n","        return ((idx - min) * (new_max - new_min) / (max - min)) + new_min\n","\n","    def get_bboxes(self, annotation):\n","        \"\"\"\n","\n","        From fasterrcnn training:\n","\n","        During training, the model expects both the input tensors and targets (list of dictionary),\n","        containing:\n","        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (Int64Tensor[N]): the class label for each ground-truth box\n","\n","        Args:\n","            annotation (_type_): _description_\n","\n","        Returns:\n","            _type_: _description_\n","        \"\"\"\n","\n","        bboxes, classes, = [], []\n","\n","        if \"object\" not in annotation.keys():\n","            return bboxes, classes\n","\n","        if type(annotation[\"object\"]) == list:\n","            object_list = annotation[\"object\"]\n","        else:\n","            object_list = [annotation[\"object\"]]\n","\n","        for bbox in object_list:\n","\n","            bbox_coords = [\n","                int(bbox[\"bndbox\"][\"xmin\"]),\n","                int(bbox[\"bndbox\"][\"ymin\"]),\n","                int(bbox[\"bndbox\"][\"xmax\"]),\n","                int(bbox[\"bndbox\"][\"ymax\"])\n","            ]\n","\n","            if(bbox_coords[0] == bbox_coords[2] or bbox_coords[1] == bbox_coords[3]):\n","              continue\n","\n","            bboxes.append(bbox_coords)\n","            classes.append(1) # all class 1, a person\n","\n","        return bboxes, classes\n","\n","    def collate_fn(self, batch):\n","\n","        rgb_imagelist = []\n","        ir_imagelist = []\n","        targets = []\n","\n","        for data in batch:\n","\n","            rgb_tensor = self.to_tensor(Image.open(data[\"rgb\"]))\n","            ir_tensor = self.to_tensor(Image.open(data[\"ir\"]))\n","\n","            if self.normalize:\n","              rgb_tensor = self.rgb_normalize(rgb_tensor)\n","              ir_tensor = self.ir_normalize(ir_tensor)\n","\n","            bboxes = data[\"bboxes\"]\n","            classes = data[\"classes\"]\n","\n","            # choose to randomly flip tensors & their bbox\n","            if self.flips and np.random.rand() < 0.5:\n","              rgb_tensor = torchvision.transforms.functional.hflip(rgb_tensor)\n","              ir_tensor = torchvision.transforms.functional.hflip(ir_tensor)\n","              # shape N, 4 [xmin, ymin, xmax, ymax]\n","              # flipping a bbox is like subtracting the width of the image from the bbox, also new min is the old max val\n","              new_min = rgb_tensor.shape[2] - bboxes[:, 2]\n","              new_max = rgb_tensor.shape[2] - bboxes[:, 0]\n","\n","              bboxes[:, 0] = new_min\n","              bboxes[:, 2] = new_max\n","\n","            # map the bboxes to the correct sizes after resize\n","            targets.append({\n","                \"boxes\": bboxes.to(DEVICE),\n","                \"labels\": classes.to(DEVICE)\n","            })\n","            rgb_imagelist.append(rgb_tensor.to(DEVICE))\n","            ir_imagelist.append(ir_tensor.to(DEVICE))\n","\n","\n","\n","        if self.partition == \"train\":\n","          return rgb_imagelist, ir_imagelist, targets\n","        else:\n","          return rgb_imagelist, ir_imagelist\n","\n","    def __getitem__(self, ind):\n","        return self.data[ind]\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3419,"status":"ok","timestamp":1714100423589,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"SgcPhIlybt8h","outputId":"3d523206-6e49-45ea-9152-7180aca99149"},"outputs":[{"name":"stderr","output_type":"stream","text":["Building train dataset:  64%|██████▎   | 6873/10822 [00:02<00:01, 3359.82it/s]"]},{"name":"stdout","output_type":"stream","text":["Skipping File 100030, with no annotations.\n","Skipping File 100033, with no annotations.\n"]},{"name":"stderr","output_type":"stream","text":["Building train dataset: 100%|██████████| 10822/10822 [00:03<00:00, 3259.01it/s]\n","Building valid dataset: 100%|██████████| 1203/1203 [00:00<00:00, 3876.58it/s]\n"]}],"source":["\n","ROOT = \"/content/data/LLVIP/\"\n","\n","llvip_train_data = LLVIPDataset(\n","    root = ROOT,\n","    # rgb_mean=[0.2226, 0.2136, 0.1458],\n","    # rgb_std=[0.1941, 0.1942, 0.1872],\n","    # ir_mean=[0.3116, 0.3116, 0.3116],\n","    # ir_std=[0.1681, 0.1681, 0.1681],\n","    flips=True\n",")\n","\n","llvip_val_data = LLVIPDataset(\n","    root = ROOT,\n","    val_set=llvip_train_data.val_set,\n","    partition=\"valid\",\n","    # rgb_mean=[0.2226, 0.2136, 0.1458],\n","    # rgb_std=[0.1941, 0.1942, 0.1872],\n","    # ir_mean=[0.3116, 0.3116, 0.3116],\n","    # ir_std=[0.1681, 0.1681, 0.1681],\n","    flips=False\n",")\n","\n","train_loader = DataLoader(\n","                llvip_train_data,\n","                batch_size=config[\"batch_size\"],\n","                collate_fn=llvip_train_data.collate_fn,\n","                shuffle=True,\n","                # num_workers=4\n",")\n","val_loader = DataLoader(\n","                llvip_val_data,\n","                batch_size=config[\"batch_size\"],\n","                collate_fn=llvip_val_data.collate_fn,\n","                shuffle=False\n","                # num_workers=4\n",")\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714100423589,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"MUx6YePNcC3V"},"outputs":[],"source":["def stack_images(image1, image2):\n","    # Determine the maximum height among the two images\n","    max_height = max(image1.size[1], image2.size[1])\n","\n","    # Create a new blank image with double the width of the input images and maximum height\n","    stacked_image = Image.new('RGB', (image1.size[0] + image2.size[0], max_height))\n","\n","    # Paste the first image onto the blank image\n","    stacked_image.paste(image1, (0, 0))\n","\n","    # Paste the second image next to the first one\n","    stacked_image.paste(image2, (image1.size[0], 0))\n","\n","    return stacked_image\n","\n","def visualize_bboxes(ind, rgb, ir, target, pred=None, _slice=False):\n","\n","    rgb_image = F.to_pil_image(rgb[ind])\n","    rgb_draw = ImageDraw.Draw(rgb_image)\n","\n","    ir_image = F.to_pil_image(ir[ind])\n","    ir_draw = ImageDraw.Draw(ir_image)\n","\n","    img_slice = None\n","    for i, bbox in enumerate(target[ind]['boxes']):\n","\n","        if not img_slice:\n","            img_slice = [int(idx) for idx in bbox]\n","\n","        ir_draw.rectangle(list(bbox), outline=(0, 255, 0))\n","        rgb_draw.rectangle(list(bbox), outline=(0, 255, 0))\n","\n","    if pred:\n","      for i, bbox in enumerate(pred[ind]['boxes']):\n","\n","        if not img_slice:\n","            img_slice = [int(idx) for idx in bbox]\n","\n","        ir_draw.rectangle(list(bbox), outline=(255, 0, 0))\n","        rgb_draw.rectangle(list(bbox), outline=(255, 0, 0))\n","\n","\n","    im_show = IPythonViewer()\n","\n","    img = stack_images(rgb_image, ir_image)\n","\n","    im_show.show(img)\n","\n","    if _slice:\n","      rgb_image_slice = F.to_pil_image(rgb[ind][:, img_slice[1]:img_slice[3], img_slice[0]:img_slice[2] ])\n","      rgb_show_slice = IPythonViewer()\n","      rgb_show_slice.show(rgb_image_slice)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":811,"output_embedded_package_id":"1MOdWBJn9ygyJC4iltESBHmZ-BoA0hNCI"},"executionInfo":{"elapsed":4370,"status":"ok","timestamp":1714100427956,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"k9KvfrAHcFPC","outputId":"477fb9ad-d4eb-4349-d3a1-ab820e61cbcb"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["for i, (rgb, ir, target) in enumerate(train_loader):\n","\n","    print(\"DATA GENERAL INFO \\n\")\n","    print(f\"Proper Values RGB: { 1.0 >= rgb[0].all() >= 0.0 } | Proper Values IR: { 1.0 >= ir[0].all() >= 0.0 }\")\n","    print(75 * \"+\")\n","    print(f\"\\n{rgb[0].size()=} | {rgb[0].device}\")\n","    print(f\"{ir[0].size()=} |  {ir[0].device}\")\n","    print(f\"{target[0]=}\\n\")\n","    print(75*\"=\")\n","\n","    visualize_bboxes(0, rgb, ir, target)\n","\n","    break"]},{"cell_type":"markdown","metadata":{"id":"7AoZhYZAUxO7"},"source":["## Backbone"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":108,"status":"ok","timestamp":1714100427956,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"rvhAMUTxcHw0"},"outputs":[],"source":["class ResNetBottleneck(torch.nn.Module):\n","\n","    def __init__(self, in_channels, reduction_channels, out_channels, stride=1):\n","        super(ResNetBottleneck, self).__init__()\n","\n","        # basic idea: reduce to inchannels to redunction channels, the back up to outchannels afterwards\n","\n","        self.conv1x1_reduction = torch.nn.Conv2d(in_channels, reduction_channels, kernel_size=1, bias=False)\n","        self.bn1 = torch.nn.BatchNorm2d(reduction_channels)\n","\n","        self.relu = torch.nn.ReLU(inplace=True)\n","\n","        self.conv3x3_process = torch.nn.Conv2d(reduction_channels, reduction_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n","        self.bn2 = torch.nn.BatchNorm2d(reduction_channels)\n","\n","        self.conv1x1_expansion = torch.nn.Conv2d(reduction_channels, out_channels, kernel_size=1, bias=False)\n","        self.bn3 = torch.nn.BatchNorm2d(out_channels)\n","\n","        self.downsample = None\n","\n","        if in_channels != out_channels or stride > 1:\n","            self.downsample = torch.nn.Sequential(\n","                torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                torch.nn.BatchNorm2d(out_channels)\n","            )\n","\n","\n","    def forward(self, x):\n","      x_in = x\n","\n","      Fx = self.conv1x1_reduction(x)\n","      Fx = self.bn1(Fx)\n","      Fx = self.relu(Fx)\n","\n","      Fx = self.conv3x3_process(Fx)\n","      Fx = self.bn2(Fx)\n","      Fx = self.relu(Fx)\n","\n","      Fx = self.conv1x1_expansion(Fx)\n","      Fx = self.bn3(Fx)\n","\n","      # if we have a stride > 1 in the first conv layer, we need to downsample\n","      # the input to have shapes match\n","      if self.downsample is not None:\n","        x_in = self.downsample(x)\n","\n","      # approx the residual function\n","      Fx = Fx + x_in\n","      Fx = self.relu(Fx)\n","\n","      return Fx\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":110,"status":"ok","timestamp":1714100427959,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"f-zJ6wf1cJlq"},"outputs":[],"source":["class ResNetStem(torch.nn.Module):\n","    # channels may also be termed as planes within the context of blocks\n","    def __init__(self, in_channels, out_channels, kernel_size=7, stride=2, padding=3, pool=3, pool_stride=2):\n","        super(ResNetStem, self).__init__()\n","\n","        # the stem of a\n","        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n","        self.bn = torch.nn.BatchNorm2d(out_channels)\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.pool = torch.nn.MaxPool2d(kernel_size=pool, stride=pool_stride, padding=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":103,"status":"ok","timestamp":1714100427959,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"_PwCIzcack1k"},"outputs":[],"source":["from importlib.util import set_package\n","import gc\n","torch.cuda.empty_cache()\n","gc.collect()\n","from collections import OrderedDict\n","\n","\n","class TresNetBottleneck(torch.nn.Module):\n","\n","    # resnet uses either bottleneck or basic blocks\n","\n","    def __init__(self, stem_in, blocks, reduction_factor, num_classes=7001):\n","        super(TresNetBottleneck, self).__init__()\n","\n","        self.stem = ResNetStem(in_channels=stem_in, out_channels=blocks[0]//reduction_factor)\n","\n","        self.stages = torch.nn.ModuleList()\n","        stage = []\n","\n","        self.out_channels = 256\n","\n","        for i in range(len(blocks)):\n","\n","            prev_ch = blocks[0] // reduction_factor if i == 0 else blocks[i-1]\n","            curr_ch = blocks[i]\n","            next_ch = blocks[i] if i == len(blocks)-1 else blocks[i+1]\n","\n","            if curr_ch != prev_ch:\n","\n","              stride = 1 if curr_ch <= 256 else 2\n","              stage.append(ResNetBottleneck(in_channels=prev_ch, reduction_channels=(curr_ch // reduction_factor), out_channels=curr_ch, stride=stride))\n","\n","            elif curr_ch != next_ch:\n","              stage.append(ResNetBottleneck(in_channels=curr_ch, reduction_channels=(curr_ch // reduction_factor), out_channels=curr_ch))\n","\n","              self.stages.append(torch.nn.Sequential(*stage))\n","              stage = []\n","\n","            else:\n","              stage.append(ResNetBottleneck(in_channels=curr_ch, reduction_channels=(curr_ch // reduction_factor), out_channels=next_ch))\n","\n","        self.stages.append(torch.nn.Sequential(*stage))\n","\n","\n","        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","        self.flatten = torch.nn.Flatten()\n","\n","        self.fc = torch.nn.Linear(blocks[-1], num_classes)\n","\n","    def forward(self, x, return_feats=False):\n","\n","        feat = self.stem(x)\n","\n","        stage_features = []\n","        for i, stage in enumerate(self.stages):\n","          feat = stage(feat)\n","\n","          if return_feats:\n","                stage_features.append(feat)\n","\n","        x = self.fc(self.flatten(self.avgpool(feat)))\n","\n","        if return_feats:\n","            return stage_features\n","        else:\n","            return x\n","\n","    def get_weights(self, model):\n","        return [param for name, param in model.named_parameters() if \"weight\" in name and \"bn\" not in name]\n","\n","    def get_biases(self, model):\n","        return [param for name, param in model.named_parameters() if \"bias\" in name and \"bn\" not in name]\n","\n","    def get_batchnorms(self, model):\n","        weights = [param for name, param in model.named_parameters() if \"bn\" in name and \"weight\" in name]\n","        biases = [param for name, param in model.named_parameters() if \"bn\" in name and \"bias\" in name]\n","        return weights + biases\n","\n","    def compare_model_components(self, model):\n","        # Compare Weights\n","        weights1 = self.get_weights(self)\n","        weights2 = self.get_weights(model)\n","\n","        self.compare_components(weights1, weights2, \"Weights\")\n","\n","        # Compare Biases\n","        biases1 = self.get_biases(self)\n","        biases2 = self.get_biases(model)\n","\n","        self.compare_components(biases1, biases2, \"Biases\")\n","\n","        # Compare BatchNorm parameters\n","        bn1 = self.get_batchnorms(self)\n","        bn2 = self.get_batchnorms(model)\n","        self.compare_components(bn1, bn2, \"BatchNorms\")\n","\n","    def compare_components(self, comp1, comp2, name):\n","        min_len = min(len(comp1), len(comp2))\n","        if len(comp1) != len(comp2):\n","            print(f\"Warning: Different number of {name} ({len(comp1)} vs {len(comp2)})\")\n","\n","        for i in range(min_len):\n","            if comp1[i].size() != comp2[i].size():\n","                print(f\"{name} {i} differ in size: {comp1[i].size()} vs {comp2[i].size()}\")\n","            else:\n","                comp2[i] = comp1[i]\n","\n","            if comp1[i].size() == comp2[i].size() and (comp1[i] == comp2[i]).all():\n","              pass\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":104,"status":"ok","timestamp":1714100427960,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"WalX81QtdQCm"},"outputs":[],"source":["class FPN(torch.nn.Module):\n","    def __init__(self, channel_sizes, feature_dim=256):\n","        super().__init__()\n","\n","        self.n = len(channel_sizes)\n","\n","        # top layer\n","        self.top_layer = torch.nn.Conv2d(channel_sizes[-1], feature_dim, kernel_size=1, stride=1, padding=0)\n","\n","        # smoothing layer\n","        self.smooth_layers = [torch.nn.Conv2d(feature_dim, feature_dim, kernel_size=3, stride=1, padding=1) for i in range(self.n)]\n","        self.smooth_layers = torch.nn.Sequential(*self.smooth_layers)\n","\n","        # lateral layers\n","        self.lateral_layers = [torch.nn.Conv2d(channel_sizes[i], feature_dim, kernel_size=1, stride=1, padding=0) for i in range(self.n-1)]\n","        self.lateral_layers = torch.nn.Sequential(*self.lateral_layers)\n","\n","\n","    def upsample_add(self, top_down_path, lat_connection):\n","      _, _, h, w = lat_connection.size()\n","      upsampled_map = torch.nn.functional.interpolate(top_down_path, size=(h,w), mode='bilinear')\n","      return upsampled_map + lat_connection\n","\n","\n","    def forward(self, c):\n","      # top-down path\n","      p = [self.top_layer(c[-1])]\n","      for i in range(self.n-2,-1,-1):\n","        p.append(self.upsample_add(p[i-self.n+1], self.lateral_layers[i](c[i])))\n","      p = list(reversed(p))\n","\n","      # smoothing\n","      for i in range(self.n):\n","        p[i] = self.smooth_layers[i](p[i])\n","\n","      return p\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":105,"status":"ok","timestamp":1714100427966,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"V9wLv33Mc3YO"},"outputs":[],"source":["from collections import OrderedDict\n","\n","class BackbonePipeline(torch.nn.Module):\n","\n","    def __init__(self, config):\n","      super().__init__()\n","\n","      # initialize config values\n","      self.num_classes = config['num_classes']\n","      self.reduction_factor = config['reduction_factor']\n","      self.channel_sizes = config['channel_sizes']\n","      self.out_channels = config['fpn_feature_dim']\n","\n","      # create channels for ResNet-50\n","      channels = []\n","      channels += [self.channel_sizes[0] for _ in range(3)]\n","      channels += [self.channel_sizes[1] for _ in range(4)]\n","      channels += [self.channel_sizes[2] for _ in range(6)]\n","      channels += [self.channel_sizes[3] for _ in range(3)]\n","\n","      # get pretrained ResNet-50 weights\n","      self.resnet50 = torch.hub.load('pytorch/vision:v0.10.0', \"resnet50\", pretrained=True)\n","\n","      # initialize ResNet-50 backbones\n","      self.backbone = TresNetBottleneck(3, channels, self.reduction_factor)\n","      self.backbone.compare_model_components(self.resnet50)\n","\n","      # initialize FPN\n","      self.fpn = FPN(self.channel_sizes, self.out_channels)\n","\n","\n","    def forward(self, images):\n","      # ResNet features\n","      # print(\"getting ResNet features\")\n","      feats = self.backbone(images, return_feats=True)\n","\n","      # pass through FPN\n","      p = self.fpn(feats)\n","\n","      # return an ordered dict\n","      feats = OrderedDict()\n","      feats['0'] = p[0]\n","      feats['1'] = p[1]\n","      feats['2'] = p[2]\n","      feats['3'] = p[3]\n","\n","\n","\n","      return feats\n"]},{"cell_type":"markdown","metadata":{"id":"7zoKHdduigg5"},"source":["## Faster-RCNN"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":65,"status":"ok","timestamp":1714100427966,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"M9IHkKlluKaX"},"outputs":[],"source":["from torchvision.ops import MultiScaleRoIAlign\n","from torchvision.models.detection.roi_heads import RoIHeads\n","from torchvision.models.detection.rpn import RegionProposalNetwork, RPNHead\n","from torchvision.models.detection.transform import GeneralizedRCNNTransform\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor\n","from typing import List, Tuple, Dict\n","import warnings\n","\n","def _default_anchorgen():\n","    anchor_sizes = ((32,), (64,), (128,), (256,))\n","    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n","    return AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","\n","class FasterRCNN(torch.nn.Module):\n","    \"\"\"\n","    Implements Faster R-CNN.\n","\n","    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n","    image, and should be in 0-1 range. Different images can have different sizes.\n","\n","    The behavior of the model changes depending on if it is in training or evaluation mode.\n","\n","    During training, the model expects both the input tensors and targets (list of dictionary),\n","    containing:\n","        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (Int64Tensor[N]): the class label for each ground-truth box\n","\n","    The model returns a Dict[Tensor] during training, containing the classification and regression\n","    losses for both the RPN and the R-CNN.\n","\n","    During inference, the model requires only the input tensors, and returns the post-processed\n","    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n","    follows:\n","        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (Int64Tensor[N]): the predicted labels for each image\n","        - scores (Tensor[N]): the scores or each prediction\n","\n","    Args:\n","        backbone (nn.Module): the network used to compute the features for the model.\n","            It should contain an out_channels attribute, which indicates the number of output\n","            channels that each feature map has (and it should be the same for all feature maps).\n","            The backbone should return a single Tensor or and OrderedDict[Tensor].\n","        num_classes (int): number of output classes of the model (including the background).\n","            If box_predictor is specified, num_classes should be None.\n","        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n","        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n","        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n","            They are generally the mean values of the dataset on which the backbone has been trained\n","            on\n","        image_std (Tuple[float, float, float]): std values used for input normalization.\n","            They are generally the std values of the dataset on which the backbone has been trained on\n","        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n","            maps.\n","        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n","        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n","        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n","        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n","        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n","        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n","        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n","            considered as positive during training of the RPN.\n","        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n","            considered as negative during training of the RPN.\n","        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n","            for computing the loss\n","        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n","            of the RPN\n","        rpn_score_thresh (float): only return proposals with an objectness score greater than rpn_score_thresh\n","        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n","            the locations indicated by the bounding boxes\n","        box_head (nn.Module): module that takes the cropped feature maps as input\n","        box_predictor (nn.Module): module that takes the output of box_head and returns the\n","            classification logits and box regression deltas.\n","        box_score_thresh (float): during inference, only return proposals with a classification score\n","            greater than box_score_thresh\n","        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n","        box_detections_per_img (int): maximum number of detections per image, for all classes.\n","        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n","            considered as positive during training of the classification head\n","        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n","            considered as negative during training of the classification head\n","        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n","            classification head\n","        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n","            of the classification head\n","        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n","            bounding boxes\n","\n","    Example::\n","\n","        >>> import torch\n","        >>> import torchvision\n","        >>> from torchvision.models.detection import FasterRCNN\n","        >>> from torchvision.models.detection.rpn import AnchorGenerator\n","        >>> # load a pre-trained model for classification and return\n","        >>> # only the features\n","        >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n","        >>> # FasterRCNN needs to know the number of\n","        >>> # output channels in a backbone. For mobilenet_v2, it's 1280,\n","        >>> # so we need to add it here\n","        >>> backbone.out_channels = 1280\n","        >>>\n","        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n","        >>> # location, with 5 different sizes and 3 different aspect\n","        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n","        >>> # map could potentially have different sizes and\n","        >>> # aspect ratios\n","        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n","        >>>\n","        >>> # let's define what are the feature maps that we will\n","        >>> # use to perform the region of interest cropping, as well as\n","        >>> # the size of the crop after rescaling.\n","        >>> # if your backbone returns a Tensor, featmap_names is expected to\n","        >>> # be ['0']. More generally, the backbone should return an\n","        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n","        >>> # feature maps to use.\n","        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","        >>>                                                 output_size=7,\n","        >>>                                                 sampling_ratio=2)\n","        >>>\n","        >>> # put the pieces together inside a FasterRCNN model\n","        >>> model = FasterRCNN(backbone,\n","        >>>                    num_classes=2,\n","        >>>                    rpn_anchor_generator=anchor_generator,\n","        >>>                    box_roi_pool=roi_pooler)\n","        >>> model.eval()\n","        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","        >>> predictions = model(x)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        backbone,\n","        num_classes=None,\n","        # transform parameters\n","        min_size=800,\n","        max_size=1333,\n","        image_mean=None,\n","        image_std=None,\n","        # RPN parameters\n","        rpn_anchor_generator=None,\n","        rpn_head=None,\n","        rpn_pre_nms_top_n_train=2000,\n","        rpn_pre_nms_top_n_test=1000,\n","        rpn_post_nms_top_n_train=2000,\n","        rpn_post_nms_top_n_test=1000,\n","        rpn_nms_thresh=0.7,\n","        rpn_fg_iou_thresh=0.7,\n","        rpn_bg_iou_thresh=0.3,\n","        rpn_batch_size_per_image=256,\n","        rpn_positive_fraction=0.5,\n","        rpn_score_thresh=0.0,\n","        # Box parameters\n","        box_roi_pool=None,\n","        box_head=None,\n","        box_predictor=None,\n","        box_score_thresh=0.05,\n","        box_nms_thresh=0.5,\n","        box_detections_per_img=100,\n","        box_fg_iou_thresh=0.5,\n","        box_bg_iou_thresh=0.5,\n","        box_batch_size_per_image=512,\n","        box_positive_fraction=0.25,\n","        bbox_reg_weights=None,\n","        **kwargs,\n","    ):\n","\n","        super().__init__()\n","\n","        if not hasattr(backbone, \"out_channels\"):\n","            raise ValueError(\n","                \"backbone should contain an attribute out_channels \"\n","                \"specifying the number of output channels (assumed to be the \"\n","                \"same for all the levels)\"\n","            )\n","\n","        self.backbone = backbone\n","\n","        if not isinstance(rpn_anchor_generator, (AnchorGenerator, type(None))):\n","            raise TypeError(\n","                f\"rpn_anchor_generator should be of type AnchorGenerator or None instead of {type(rpn_anchor_generator)}\"\n","            )\n","        if not isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None))):\n","            raise TypeError(\n","                f\"box_roi_pool should be of type MultiScaleRoIAlign or None instead of {type(box_roi_pool)}\"\n","            )\n","\n","        if num_classes is not None:\n","            if box_predictor is not None:\n","                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n","        else:\n","            if box_predictor is None:\n","                raise ValueError(\"num_classes should not be None when box_predictor is not specified\")\n","\n","        out_channels = backbone.out_channels\n","\n","        if rpn_anchor_generator is None:\n","            rpn_anchor_generator = _default_anchorgen()\n","        # if rpn_head is None:\n","        rpn_head = RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])\n","\n","        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n","        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n","\n","        self.rpn = RegionProposalNetwork(\n","            rpn_anchor_generator,\n","            rpn_head,\n","            rpn_fg_iou_thresh,\n","            rpn_bg_iou_thresh,\n","            rpn_batch_size_per_image,\n","            rpn_positive_fraction,\n","            rpn_pre_nms_top_n,\n","            rpn_post_nms_top_n,\n","            rpn_nms_thresh,\n","            score_thresh=rpn_score_thresh,\n","        )\n","\n","        if box_roi_pool is None:\n","            box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)\n","\n","        if box_head is None:\n","            resolution = box_roi_pool.output_size[0]\n","            representation_size = 1024\n","            box_head = TwoMLPHead(out_channels * resolution**2, representation_size)\n","\n","        if box_predictor is None:\n","            representation_size = 1024\n","            box_predictor = FastRCNNPredictor(representation_size, num_classes)\n","\n","        self.roi_heads = RoIHeads(\n","            # Box\n","            box_roi_pool,\n","            box_head,\n","            box_predictor,\n","            box_fg_iou_thresh,\n","            box_bg_iou_thresh,\n","            box_batch_size_per_image,\n","            box_positive_fraction,\n","            bbox_reg_weights,\n","            box_score_thresh,\n","            box_nms_thresh,\n","            box_detections_per_img,\n","        )\n","\n","        if image_mean is None:\n","            image_mean = [0, 0, 0]\n","        if image_std is None:\n","            image_std = [ 1, 1, 1 ]\n","\n","        self.transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std, fixed_size=(640, 512), **kwargs)\n","\n","    def eager_outputs(self, losses, detections):\n","        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Union[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","        if self.training:\n","            return losses\n","\n","        return detections\n","\n","    def forward(self, images, targets=None):\n","        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","        \"\"\"\n","        Args:\n","            images (list[Tensor]): rgb images to be processed\n","            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","\n","        Returns:\n","            result (list[BoxList] or dict[Tensor]): the output from the model.\n","                During training, it returns a dict[Tensor] which contains the losses.\n","                During testing, it returns list[BoxList] contains additional fields\n","                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","\n","        \"\"\"\n","        if self.training:\n","            if targets is None:\n","                torch._assert(False, \"targets should not be none when in training mode\")\n","            else:\n","                for target in targets:\n","                    boxes = target[\"boxes\"]\n","                    if isinstance(boxes, torch.Tensor):\n","                        torch._assert(\n","                            len(boxes.shape) == 2 and boxes.shape[-1] == 4,\n","                            f\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\",\n","                        )\n","                    else:\n","                        torch._assert(False, f\"Expected target boxes to be of type Tensor, got {type(boxes)}.\")\n","\n","        original_image_sizes: List[Tuple[int, int]] = []\n","        for img in images:\n","            val = img.shape[-2:]\n","            torch._assert(\n","                len(val) == 2,\n","                f\"expecting the last two dimensions of the Tensor to be H and W instead got {img.shape[-2:]}\",\n","            )\n","            original_image_sizes.append((val[0], val[1]))\n","\n","        images, targets = self.transform(images, targets)\n","\n","        # Check for degenerate boxes\n","        # TODO: Move this to a function\n","        if targets is not None:\n","            for target_idx, target in enumerate(targets):\n","                boxes = target[\"boxes\"]\n","                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n","                if degenerate_boxes.any():\n","                    # print the first degenerate box\n","                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                    degen_bb: List[float] = boxes[bb_idx].tolist()\n","                    torch._assert(\n","                        False,\n","                        \"All bounding boxes should have positive height and width.\"\n","                        f\" Found invalid box {degen_bb} for target at index {target_idx}.\",\n","                    )\n","\n","        features = self.backbone(images.tensors)\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","        proposals, proposal_losses = self.rpn(images, features, targets)\n","        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n","        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","        losses = {}\n","        losses.update(detector_losses)\n","        losses.update(proposal_losses)\n","\n","        return losses, detections"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["48b648bd83ed47f8ababb259b11c11e2","b160c66e65604961a5c274e1141ed362","dab55ba3ccca4316a050319de159097f","5490024a463e413b83142b2bd6e4b15a","d2e150925145404f82cedb093a4d3705","660265849a464bd98228cdd1588f0e43","700d239059474f7197b59177f68687c8","94cdee315cd74994ae64171f729acaf6","a8eeb9ba7e0045bea30b6a95058df41d","d508511543e847d4901be93841be5a18","f1b6f5994c5d461e8819d7cc959077b4"]},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1714100428461,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"fFS-69_tiikh","outputId":"7704e8ae-57d9-40e4-b99b-43ccf3b0c404"},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n","\n","\n","# MAKE SURE YOUR BACKBONE ALIGNS WITH THE BACKBONE YOU CHOISE IN MODEL-TRAIN, pytorch or custom\n","\n","# custom backbone\n","# backbone =  BackbonePipeline(config)\n","\n","# anchor_generator = AnchorGenerator(\n","#     sizes=((32,), (64,), (128,), (256,)),\n","#     aspect_ratios=((0.5, 1.0, 2.0),) * 4\n","# )\n","\n","# pytorch backbone\n","backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=((32,), (64,), (128,), (256,), (512,)),\n","    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",")\n","\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","model = FasterRCNN(backbone,\n","                   num_classes=config['num_classes'],\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)\n","model.to(DEVICE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1714100428461,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"QvvYAV7spKLE","outputId":"f3e020f8-4fba-41d8-8433-ec640d9bd35e"},"outputs":[],"source":["from torchinfo import summary\n","for i, (rgb, ir, target) in enumerate(train_loader):\n","  print(summary(model, input_data=(ir, target)))\n","  break"]},{"cell_type":"markdown","metadata":{"id":"vmTMjvBHjA1q"},"source":["## Training and Experiments"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1714100428462,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"1vK_1iLVevVO"},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":54,"status":"ok","timestamp":1714100428462,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"dL3U6HllexDx"},"outputs":[],"source":["def train(model, dataloader, optimizer):\n","\n","  progress_bar = tqdm.tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Training')\n","  rpn_loss = frcnn_loss = 0.0\n","\n","  model.train()\n","\n","  for i, (rgb, ir, targets) in enumerate(dataloader):\n","\n","    optimizer.zero_grad()\n","\n","    # no detections during training\n","    losses, _ = model(rgb, targets)\n","\n","    # need to weight regression loss appropriately in rpn, will do the same with faster_rcnn until further notice\n","    # classification is normalized by the batch size (256), reg is normalized by the number of anchors (~2400),\n","    # factor of 10 makes then roughly equivalent\n","\n","    rpn_losses = losses[\"loss_objectness\"] + config['rpn_gamma']*losses['loss_rpn_box_reg']\n","    fastercnn_losses = losses[\"loss_classifier\"] + config['fcnn_gamma']*losses['loss_box_reg']\n","\n","    # tr\n","    loss = rpn_losses + fastercnn_losses\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    rpn_loss += rpn_losses.item()\n","    frcnn_loss += fastercnn_losses.item()\n","\n","    progress_bar.set_postfix(rpn_loss=\"{:.04f}\".format(float(rpn_loss / (i + 1))),\n","                              frcnn_loss=\"{:.04f}\".format(float(frcnn_loss / (i + 1))))\n","\n","    progress_bar.update()\n","\n","  avg_rpn_loss = rpn_loss / len(dataloader)\n","  avg_frcnn_loss = frcnn_loss / len(dataloader)\n","\n","  return avg_rpn_loss, avg_frcnn_loss\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":1756,"status":"ok","timestamp":1714100430164,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"madPGLcmeymM"},"outputs":[],"source":["from torchmetrics.detection import MeanAveragePrecision\n","import time\n","\n","def eval(model, dataloader):\n","\n","  progress_bar = tqdm.tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Validation')\n","  rpn_loss = frcnn_loss = 0.0\n","\n","  model.eval()\n","  metric = MeanAveragePrecision(iou_type=\"bbox\")\n","\n","  time_start = time.time()\n","\n","  for i, (rgb, ir, targets) in enumerate(dataloader):\n","\n","    with torch.inference_mode():\n","      _, detections = model(rgb, targets)\n","\n","    metric.update(detections, targets)\n","\n","    progress_bar.update()\n","\n","  total_time = time.time() - time_start\n","  avg_inf_time = total_time / ( len(dataloader) * config[\"batch_size\"] )\n","\n","  metric_vals = metric.compute()\n","\n","  return metric_vals, avg_inf_time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"elapsed":11362,"status":"ok","timestamp":1714100441523,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"xKqf4W_le0x0","outputId":"7a707ca2-15cf-4cf2-e22a-8b25ada35455"},"outputs":[],"source":["wandb.login(key=\"FILL-ME-IN\")\n","# Create your wandb run\n","run = wandb.init(\n","    name = config['run_name'],\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # run_id = ### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"FILL-ME-IN\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["94b7245599bf44198b5e60ea6553aa8a","cdd8b684cf9346188072e7eac339a8ae","a6c71cbc57094ac0aeec448513b4dd86","17ad516f7f5a4ecb9fb5d42124ecd7cb","2cb15d2457144f47abc4f118843b89c2","c695fa3b1f084059b7df3652305566db","038f058563d942078bf8c4e83c4c7d9a","b49ee31ee8754767ab40a12e7ce7f7ae"]},"executionInfo":{"elapsed":9114154,"status":"ok","timestamp":1714109555660,"user":{"displayName":"Pepper Person","userId":"09833370125675002183"},"user_tz":240},"id":"qipgeUsle2cG","outputId":"b0f35909-5eba-4289-acf1-62a54d3bee6e"},"outputs":[],"source":["best_mAP = config['best_mAP']\n","checkpoints_path = config['checkpoints_path']\n","run_name = config['run_name']\n","\n","if not os.path.exists(checkpoints_path):\n","  raise ValueError(\"Path does not exist!\")\n","\n","for epoch in range(config['epochs']):\n","\n","    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n","\n","    curr_lr = float(optimizer.param_groups[0]['lr'])\n","\n","    tr_rpn_loss, tr_frcnn_loss = train(model, train_loader, optimizer)\n","\n","    print(\"\\nEpoch {}/{}: \\nTrain Loss (RPN) {:.04f}\\t Train Loss (Faster-RCNN) {:.04f}\".format(\n","        epoch + 1, config['epochs'], tr_rpn_loss, tr_frcnn_loss))\n","\n","    metrics, inf_time = eval(model, val_loader)\n","    print(\"Val mAP {:.02f}\".format(metrics['map']*100))\n","\n","    wandb.log({\"train_rpn_loss\": tr_rpn_loss,\n","               \"train_frcnn_loss\": tr_frcnn_loss,\n","               \"val_mAP\": metrics['map'],\n","               \"val_mAP_50\": metrics['map_50'],\n","               \"val_mAP_75\": metrics['map_75'],\n","               \"avg_inf_time\": inf_time})\n","\n","    if metrics['map'] > best_mAP:\n","        best_mAP = metrics['map']\n","        torch.save({'model_state_dict':model.state_dict(),\n","                    'optimizer_state_dict':optimizer.state_dict(),\n","                    'epoch': epoch},\n","                    os.path.join(f'{checkpoints_path}/{run_name}.pth'))\n","        wandb.save(f'{run_name}.pth')\n","        print(\"Saved best model\")\n","\n","\n","run.finish()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"038f058563d942078bf8c4e83c4c7d9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17ad516f7f5a4ecb9fb5d42124ecd7cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cb15d2457144f47abc4f118843b89c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b648bd83ed47f8ababb259b11c11e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b160c66e65604961a5c274e1141ed362","IPY_MODEL_dab55ba3ccca4316a050319de159097f","IPY_MODEL_5490024a463e413b83142b2bd6e4b15a"],"layout":"IPY_MODEL_d2e150925145404f82cedb093a4d3705"}},"5490024a463e413b83142b2bd6e4b15a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d508511543e847d4901be93841be5a18","placeholder":"​","style":"IPY_MODEL_f1b6f5994c5d461e8819d7cc959077b4","value":" 97.8M/97.8M [00:00&lt;00:00, 260MB/s]"}},"660265849a464bd98228cdd1588f0e43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"700d239059474f7197b59177f68687c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94b7245599bf44198b5e60ea6553aa8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cdd8b684cf9346188072e7eac339a8ae","IPY_MODEL_a6c71cbc57094ac0aeec448513b4dd86"],"layout":"IPY_MODEL_17ad516f7f5a4ecb9fb5d42124ecd7cb"}},"94cdee315cd74994ae64171f729acaf6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6c71cbc57094ac0aeec448513b4dd86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_038f058563d942078bf8c4e83c4c7d9a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b49ee31ee8754767ab40a12e7ce7f7ae","value":1}},"a8eeb9ba7e0045bea30b6a95058df41d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b160c66e65604961a5c274e1141ed362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_660265849a464bd98228cdd1588f0e43","placeholder":"​","style":"IPY_MODEL_700d239059474f7197b59177f68687c8","value":"100%"}},"b49ee31ee8754767ab40a12e7ce7f7ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c695fa3b1f084059b7df3652305566db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdd8b684cf9346188072e7eac339a8ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb15d2457144f47abc4f118843b89c2","placeholder":"​","style":"IPY_MODEL_c695fa3b1f084059b7df3652305566db","value":"0.018 MB of 0.018 MB uploaded\r"}},"d2e150925145404f82cedb093a4d3705":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d508511543e847d4901be93841be5a18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dab55ba3ccca4316a050319de159097f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_94cdee315cd74994ae64171f729acaf6","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8eeb9ba7e0045bea30b6a95058df41d","value":102530333}},"f1b6f5994c5d461e8819d7cc959077b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
